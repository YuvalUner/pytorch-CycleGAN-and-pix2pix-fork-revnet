{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An extension utilizing the original CycleGAN architecture, but replacing the generator with a Reversible Residual Network (RevNet) generator, using the architecture presented here: https://arxiv.org/abs/1707.04585, in the paper \"Gomez, Aidan N., et al. \"The reversible residual network: Backpropagation without storing activations.\" Advances in neural information processing systems 30 (2017).\".",
   "id": "c665f91d4cfee4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install",
   "id": "17573b8ec6ee660f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git clone https://github.com/YuvalUner/pytorch-CycleGAN-and-pix2pix-fork-revnet",
   "id": "b351392a188c65b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix-fork-revnet/')"
   ],
   "id": "5676a204dd71db87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -r requirements.txt",
   "id": "b414a2d1bb076898"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets provided by the original authors with:\n",
    "\n",
    "-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images.\n",
    "\n",
    "-   Create a dataset folder under `/dataset` for your dataset.\n",
    "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
   ],
   "id": "40b1c8c3087645fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!bash ./datasets/download_cyclegan_dataset.sh horse2zebra",
   "id": "604d95d83185fae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model revnet_gan --netG revnet --ngf 128 --revnet_use_reconstruction True`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size.\n",
    "\n",
    "You can choose between `revnet_gan` and `revnet_bi_gan` for the `--model` flag. The former uses a single discriminator and is trained on the A->B direction only, relying on the reversible nature of the generator to generate B->A images. The latter uses two discriminators and is trained on both directions.\n",
    "\n",
    "You can also choose between `revnet`, `revnet_bi` and `revnet_pure` for the `--netG` flag. The first uses a RevNet generator with an initial and final convolutional layer that change the number of channels to that in the `--ngf` flag. The second uses different initial and final convolutional layers for each direction. The third uses a RevNet generator with no initial or final convolutional layers, only using RevNet blocks.\n",
    "\n",
    "You can change the kernel size in the initial and final convolutional layers with the `--revnet_G_kernel_size` flag. The default is 1.\n",
    "\n",
    "You can change the number of RevNet blocks by changing the `--n_blocks` flag. The default is 9.\n",
    "\n",
    "Finally, you can change the dimension by which the RevNet blocks are split with the `--revnet_G_split_dim` flag. The default is channel, but you can also choose height or width.\n",
    "When using `revnet_pure`, this change is a must, as splitting by channel is not possible due to the odd number of channels in the input image."
   ],
   "id": "2e7f44debe217afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model revnet_gan --netG revnet --ngf 128 --display_id -1 --revnet_use_reconstruction True",
   "id": "b8cbdd968c25201d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot datasets/horse2zebra --name horse2zebra --model revnet --no_dropout --ngf 128 --netG revnet`\n",
    "\n",
    "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
    "\n",
    "> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
   ],
   "id": "1078d5e4909b7777"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python test.py --dataroot datasets/horse2zebra --name horse2zebra --model revnet_gan --no_dropout --ngf 128 --netG revnet",
   "id": "9e97d48305017acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize",
   "id": "9bce391c8ae561d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra/test_latest/images/n02381460_1010_real_A.png')\n",
    "plt.imshow(img)"
   ],
   "id": "c67ade33f2e2bb52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra/test_latest/images/n02381460_1010_fake_B.png')\n",
    "plt.imshow(img)"
   ],
   "id": "df2406de4d795670"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra/test_latest/images/n02381460_2050_real_B.png')\n",
    "plt.imshow(img)"
   ],
   "id": "5d2a0a6909585984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra/test_latest/images/n02381460_2050_fake_A.png')\n",
    "plt.imshow(img)"
   ],
   "id": "ea8edf07bf7d2eaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
